---
title: "Problem Set 5"
format: html
---


1
30538 Problem Set 5: Web Scraping
Peter Ganong, Maggie Shi, Akbar Saputra, and Will Pennington
2024-11-03
Due 11/9 at 5:00PM Central. Worth 100 points + 10 points extra credit.
Submission Steps (10 pts)


1. This problem set is a paired problem set.
2. Play paper, scissors, rock to determine who goes first. Call that person Partner 1.
• Partner 1 (Daniel Avila, davila2020):
• Partner 2 (Nasser Alshaya, alshaya):
3. Partner 1 will accept the ps5 and then share the link it creates with their partner. You
can only share it with one partner so you will not be able to change it after your partner
has accepted.
4. “This submission is our work alone and complies with the 30538 integrity policy.” Add
your initials to indicate your agreement: **DA** **NA**
5. “I have uploaded the names of anyone else other than my partner and I worked with on
the problem set here” (1 point)
6. Late coins used this pset: **0** Late coins left after submission: **2**
7. Knit your ps5.qmd to an PDF file to make ps5.pdf,
• The PDF should not be more than 25 pages. Use head() and re-size figures when
appropriate.
8. (Partner 1): push ps5.qmd and ps5.pdf to your github repo.
9. (Partner 1): submit ps5.pdf via Gradescope. Add your partner on Gradescope.
10. (Partner 1): tag your submission in Gradescope

```{python}
import pandas as pd
import altair as alt
import time
import requests
from bs4 import BeautifulSoup

import warnings 
warnings.filterwarnings('ignore')
alt.renderers.enable("png")

url = "https://oig.hhs.gov/fraud/enforcement/"
response = requests.get(url)
soup = BeautifulSoup(response.text, 'lxml')
soup.text[0:50]
```


1     (30 points) Step 1: Develop initial scraper and crawler

1. (Partner 1) Scraping: Go to the first page of the HHS OIG’s “Enforcement Actions”
page and scrape and collect the following into a dataset:
• Title of the enforcement action
• Date
• Category (e.g, “Criminal and Civil Actions”)
• Link associated with the enforcement action
Collect your output into a tidy dataframe and print its head.

```{python}
tag = soup.find_all("ul")
ul_children = soup.find_all(lambda t: t.name == 'ul' and t.find_all("div"))

# title of enforcement action
ul_children_titles = soup.find_all(lambda t: t.name == 'ul' and t.find_all("a"))

titles_list = []
for title in ul_children_titles:
    for a in title.find_all("a"):
        text = a.get_text()
        titles_list.append(text)

     ## selecting only the titles, since the previous version of titles_list has extra text
titles_list = titles_list[136:156]

# date
ul_children_date = soup.find_all(lambda t: t.name == 'div' and t.find_all("span"))

dates_list = []
for date in ul_children_date:
    for span in date.find_all("span"):
        date = span.get_text()
        dates_list.append(date)

dates_list = dates_list[33:53]


# category
ul_children_category = soup.find_all(lambda t: t.name == 'ul' and t.find_all("span") and t.find_all("li"))

category_list = []
for category in ul_children_category:
    for li in category.find_all("li"):
        category = li.get_text("")
        category_list.append(category)

for i in range(len(category_list)):
    category_list[i] = category_list[i].replace("\n","")

category_list = category_list[74:135]

string_check = ["Criminal and Civil Actions", "State Enforcement Agencies"]

category_list_new = []
for i in range(len(category_list)):
    if category_list[i] == string_check[0] or category_list[i] == string_check[1]:
        category_list_new.append(category_list[i])
    else:
        pass

#links
links_list = []
for item in ul_children:
    for a in item.find_all('a'):
        links_list.append(a["href"])

df = pd.DataFrame({
    "ea_titles": titles_list,
    "dates": dates_list,
    "categories": category_list_new,
    "links": links_list})

df.head()
```


2. (Partner 1) Crawling: Then for each enforcement action, click the link and collect
the name of the agency involved (e.g., for this link, it would be U.S. Attorney’s Oﬀice,
Eastern District of Washington).
Update your dataframe with the name of the agency and print its head.
Hint: if you go to James A. Robinson’s profile page at the Nobel Prize website here, right-
click anywhere along the line “Aﬀiliation at the time of the award: University of Chicago,
Chicago, IL, USA”, and select Inspect, you’ll see that this aﬀiliation information is located at
the third <p> tag out of five <p> tags under the <div class="content">. Think about how
you can select the third element of <p> out of five <p> elements so you’re sure you scrape the
aﬀiliation information, not other. This way, you can scrape the name of agency to answer this
question.


2      (30 points) Making the scraper dynamic

1. Turning the scraper into a function: You will write a function that takes as input
a month and a year, and then pulls and formats the enforcement actions like in Step 1
starting from that month+year to today.

• This function should first check that the year inputted >= 2013 before starting to
scrape. If the year inputted < 2013, it should print a statement reminding the user
to restrict to year >= 2013, since only enforcement actions after 2013 are listed.

• It should save the dataframe output into a .csv file named as “enforcement_actions_
year_month.csv” (do not commit this file to git)

• If you’re crawling multiple pages, always add 1 second wait before going to the next
page to prevent potential server-side block. To implement this in Python, you may
look up .sleep() function from time library.

a. (Partner 2) Before writing out your function, write down pseudo-code of the steps that
your function will go through. If you use a loop, discuss what kind of loop you will use
and how you will define it.

```
Step 0: visually inspect the page to determine what need scrapping.
Step 1: define constant variables needed: source_link, start_date, 
Step 2: set condition to assure input year larger than 2013
Step 3: using a for loop, iterate over the range of pages based on f-string for the link of next page.
Step 4: the loop will scrape every page and extract the data needed to build the dataset basec an their tags and classes and add them to an empty dictionary which will the results of ecah iteration will be appended to an empty list to collect all data in one object; an if condition to break the loop based on strating date is included as well.
Step 5: the function stores the resulted dataset and then converts it to a csv file.
```

b. (Partner 2) Now code up your dynamic scraper and run it to start collecting the enforce-
ment actions since January 2023. How many enforcement actions do you get in your final
dataframe? What is the date and details of the earliest enforcement action it scraped?

```
I ended up with 1553 enforcement actions and the details of the earliest one are as the following:
Title: Podiatrist Pays $90,000 To Settle False Billin..
Date: January 3, 2023
Category: Criminal and Civil Actions
Agency: U.S. Attorney’s Office, Southern District of Texas
```

```{python}
import datetime
def scrape_hhs_oig(year, month):
    """
    Scrapes the HHS OIG's Enforcement Actions page for a given year and month.

    Args:
        year (int): The year to filter by.
        month (int): The month to filter by.

    Returns:
        list: A dataframe containing the scraped data.
    """
    source_link = "https://oig.hhs.gov"
    start_date = datetime.datetime(year, month, 1)
    if year < 2013:
        print("Please input a year greater than or equal to 2013.")
        return
    enforcements =[]
    # Crawling:
    for i in range(480):
        link = f'https://oig.hhs.gov/fraud/enforcement/?page={i}'
        response = requests.get(link)
        soup = BeautifulSoup(response.text, 'lxml')
        
        for li in soup.find_all('li', class_='usa-card'):
            item = {}
        # Extract title and link
            h2 = li.find('h2', class_='usa-card__heading')
            a_tag = h2.find('a', href=True)
            item['title'] = a_tag.get_text(strip=True)
            item['link'] = source_link + a_tag['href']
            # Extract date and category
            date_span = li.find('span', class_='text-base-dark')
            action_date_str = date_span.get_text(strip=True)
            action_date = datetime.datetime.strptime(action_date_str, '%B %d, %Y')
            if action_date < start_date:
                df_enforcements = pd.DataFrame(enforcements)
                df_enforcements.to_csv('enforcement_actions_year_month.csv',index = False)
                return df_enforcements
            else:
                item['date'] = action_date_str
            # Extract categories:
            category_li = li.find('ul',
             class_ = 'display-inline add-list-reset')
            item['category'] = category_li.get_text(strip=True)
            enforcements.append(item)
            # Extract Agency:
            agency_response = requests.get(item['link'])
            agency_soup = BeautifulSoup(agency_response.text, 'lxml')
            for li in agency_soup.find_all('article',class_ ='grid-col desktop:grid-col-10 margin-bottom-2 desktop:margin-bottom-5'):
                agency_li = li.find_all('li')[1]
                agency_name = agency_li.text.replace(
                    'Agency:', '').strip()
                item["Agency"] = agency_name
                break
        # Add a 1-second delay between requests
        time.sleep(1)
    # Creating dataframe of enforcements:
    df_enforcements = pd.DataFrame(enforcements)
    df_enforcements.to_csv('enforcement_actions_year_month.csv',
    index = False)
    return df_enforcements

```

```{python}
# Details of earliest enforcement action scrapped, commented out to speed up knitting
#scrape_hhs_oig(2023, 1).tail(1)
```

c. (Partner 1) Now, let’s go a little further back. Test your partner’s code by collecting
the actions since January 2021. Note that this can take a while. How many enforcement
actions do you get in your final dataframe? What is the date and details of the earliest
enforcement action it scraped? Use the dataframe from this process for every question
after this.



```{python}
# Commented out to speed knitting, a csv file extracted to be used 
#df_2021 = scrape_hhs_oig(2021,1)
```

Hint:
a. If you go to the next page in this HHS OIG’s “Enforcement Actions” page, you’ll notice
a pattern:
• Second page URL: https://oig.hhs.gov/fraud/enforcement/?page=2
• Third page URL: https://oig.hhs.gov/fraud/enforcement/?page=3
• and so on ...
b. Write a pseudo-code to help you think about how to make the crawler dynamic. You
need to loop through all the pages in the website. Hint: Note that a simple for loop
may not be suﬀicient for what this crawler requires. Use online resources to look into
different types of loops or different ways of using for loops to see if there is something
that is more appropriate for this task.


3       (15 points) Plot data based on scraped data (using altair)

1. (Partner 2) Plot a line chart that shows: the number of enforcement actions over
time (aggregated to each month+year) overall since January 2021.

```{python}
import os
base_path = r'/Users/nasser.alshaya/Desktop/Fall-2024/PPHA-30538/Repos/Nasser_Daniel_pset5'
path_data = os.path.join(base_path,'enforcement_actions_year_month.csv')
df_2021 = pd.read_csv(path_data)
line_chart = alt.Chart(df_2021).mark_line(point=True).encode(
    x=alt.X('yearmonth(date):T', title = 'Month/Year',
     axis =alt.Axis(tickCount = 48, labelAngle = -60)),
    y=alt.Y('count():Q', title='Number of Enforcement Actions'),
    tooltip=[alt.Tooltip('yearmonth(date):T', title='Month/Year'),
             alt.Tooltip('count():Q', title='Enforcement Count')]
).properties(
    title='Number of Enforcement Actions Over Time (Aggregated by Month)',
    width=800,
    height=400
)
line_chart
```

2. (Partner 1) Plot a line chart that shows: the number of enforcement actions split
out by:
• “Criminal and Civil Actions” vs. “State Enforcement Agencies”
• Five topics in the “Criminal and Civil Actions” category: “Health Care Fraud”,
“Financial Fraud”, “Drug Enforcement”, “Bribery/Corruption”, and “Other”. Hint:
You will need to divide the five topics manually by looking at the title and assigning
the relevant topic. For example, if you find the word “bank” or “financial” in the
title of an action, then that action should probably belong to “Financial Fraud”
topic.

4:         (15 points) Create maps of enforcement activity
For these questions, use this US Attorney District shapefile (link) and a Census state shapefile
(link)

1. (Partner 1) Map by state: Among actions taken by state-level agencies, clean the state
names you collected and plot a choropleth of the number of enforcement actions for each
state. Hint: look for “State of” in the agency info!

2. (Partner 2) Map by district: Among actions taken by US Attorney District-level agen-
cies, clean the district names so that you can merge them with the shapefile, and then
plot a choropleth of the number of enforcement actions in each US Attorney District.
Hint: look for “District” in the agency info.

```{python}
import geopandas as gpd

filepath = '/Users/nasser.alshaya/Desktop/Fall-2024/PPHA-30538/Repos/Nasser_Daniel_pset5/data'
path_districts = os.path.join(
    filepath,'geo_export_3d06657f-30d6-461b-af00-153ad6e863c9.shp')
path_census = os.path.join(filepath,'cb_2018_us_cd116_500k.shp')
df_census = gpd.read_file(path_census)
df_districts = gpd.read_file(path_districts)

# Cleaning district names 
df_2021['Agency'] = df_2021['Agency'].apply(
    lambda x: x.split(',', 1)[1].strip() if isinstance(x, str) and ',' in x 
    else x)

df_grouped = df_2021.groupby('Agency').size().reset_index(name='count')

df_merged = df_districts.merge(df_grouped, left_on = 'name' ,right_on='Agency',how = 'inner')
```


```{python}
df_merged = df_grouped.merge(df_districts, left_on = 'Agency' ,right_on='name',how = 'inner')
```

```{python}
import matplotlib.pyplot as plt

fig, ax = plt.subplots(1, 1, figsize=(12, 8))
df_merged.plot(column='count', ax=ax, legend=True)
ax.set_title('Number of Enforcement Actions by US Attorney District')
# To zoom in for a larger map
ax.set_xlim(-180 ,-50)
for idx, row in df_merged.iterrows():
    centroid = row['geometry'].centroid
    ax.text(centroid.x, centroid.y, str(row['count']), 
            fontsize=6, ha='center', va='center', color='white')
ax.axis('off')
plt.show()
```

(10 points) Extra credit: Calculate the enforcement actions on a per-capita basis
(Both partners can work together)

1. Use the zip code shapefile from the previous problem set and merge it with zip code-
level population data. (Go to Census Data Portal, select “ZIP Code Tabulation Area”,
check “All 5-digit ZIP Code Tabulation Areas within United States”, and under “P1
TOTAL POPULATION” select “2020: DEC Demographic and Housing Characteristics”.
Download the csv.).

2. Conduct a spatial join between zip code shapefile and the district shapefile, then aggre-
gate to get population in each district.

3. Map the ratio of enforcement actions in each US Attorney District. You can calculate the
ratio by aggregating the number of enforcement actions since January 2021 per district,
and dividing it with the population data.


